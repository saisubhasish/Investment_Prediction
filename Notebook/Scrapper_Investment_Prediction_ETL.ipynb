{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c10c613",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "save_file_to_path = \"D:/FSDS-iNeuron/10.Projects-DS/Investment_Prediction/raw_dataset/\"\n",
    "start_date = '2016-03-19'\n",
    "end_date = '2023-03-19'\n",
    "driver_path = r\"D:\\FSDS-iNeuron\\10.Projects-DS\\Investment_Prediction\\selenium\\chromedriver.exe\"\n",
    "company_list = ['britannia-industries', 'itc', 'reliance-industries', 'tata-motors-ltd', 'tata-consultancy-services']\n",
    "\n",
    "# britannia-industries --> Britannia Inductries\n",
    "# itc --> ITC\n",
    "# reliance-industries --> Reliance Industries   \n",
    "# tata-motors-ltd --> TATA Motors\n",
    "# tata-consultancy-services  -->  TCS\n",
    "\n",
    "class Data_scraper:\n",
    "    @staticmethod\n",
    "    def scraper(company, start_date, end_date, driver_path):\n",
    "        '''\n",
    "        To automate the process of changing start date and end date in the code, \n",
    "        We can create a function that takes start date and end date as input parameters and formats them as Unix timestamps. \n",
    "        We can then construct the URL with the formatted timestamps as query parameters.\n",
    "        '''\n",
    "        # logging.info(\"Step 1: Create a session and load the page\")\n",
    "        # logging.info(\"Preparing time-stamp for start and end interval\")\n",
    "        start_timestamp = int(time.mktime(time.strptime(start_date, '%Y-%m-%d')))\n",
    "        end_timestamp = int(time.mktime(time.strptime(end_date, '%Y-%m-%d')))\n",
    "\n",
    "        # logging.info(\"Preparing url with company name and time interva\")\n",
    "        url = f'https://in.investing.com/equities/{company}-historical-data?end_date={end_timestamp}&st_date={start_timestamp}'\n",
    "        \n",
    "        # logging.info(f\"Requesting {company} company url in Chrome browser\")\n",
    "        try:\n",
    "            driver = webdriver.Chrome(driver_path)\n",
    "            driver.get(url)\n",
    "\n",
    "        except Exception as e:\n",
    "            raise InvestmentPredictionException(e, sys)\n",
    "        \n",
    "        driver.implicitly_wait(2)\n",
    "        \n",
    "        # logging.info(\"Scrolling to the end of the page\")\n",
    "        driver.maximize_window()\n",
    "        driver.implicitly_wait(2) \n",
    "        \n",
    "        driver.execute_script(\"window.scrollBy(0,500)\",\"\")\n",
    "        time.sleep(3)\n",
    "        \n",
    "        driver.execute_script(\"window.scrollBy(0,2000)\",\"\")\n",
    "        time.sleep(3)\n",
    "        \n",
    "        driver.execute_script(\"window.scrollBy(0,5000)\",\"\")\n",
    "        time.sleep(3)\n",
    "        \n",
    "        driver.execute_script(\"window.scrollBy(0,5000)\",\"\")\n",
    "        time.sleep(3)\n",
    "        \n",
    "        driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "        \n",
    "        driver.execute_script(\"window.scrollBy(0,-2000)\",\"\")\n",
    "        time.sleep(5)\n",
    "\n",
    "        # logging.info(\"Step 2: Close the pop-up if it appears\")\n",
    "        try:\n",
    "            maybe_later_button = driver.find_element_by_xpath(\"//button[contains(text(),'Maybe later')]\")\n",
    "            maybe_later_button.click()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # logging.info(\"Step 3: Parse lxml code and grab tables with Beautiful Soup\")\n",
    "        try:\n",
    "            soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "            tables = soup.find_all('table')\n",
    "        except Exception as e:\n",
    "            raise InvestmentPredictionException(e, sys)\n",
    "\n",
    "        # logging.info(\"Step 4: Read tables with Pandas read_html()\")\n",
    "        dfs = pd.read_html(str(tables))\n",
    "\n",
    "        print(f'Total tables: {len(dfs)}')\n",
    "        print(dfs[0])\n",
    "\n",
    "        driver.close()\n",
    "\n",
    "        # logging.info(\"Saving scrapped data to directory\")\n",
    "        dfs[0].to_csv(f'{save_file_to_path}{company}.csv')\n",
    "\n",
    "# Function call\n",
    "list(map(lambda company: Data_scraper.scraper(company, start_date, end_date, driver_path), company_list))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
